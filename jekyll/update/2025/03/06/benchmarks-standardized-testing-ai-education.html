<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>AI Benchmarks: Are We Juking the Stats? | Mark Pettyjohn</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="AI Benchmarks: Are We Juking the Stats?" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Writing is thinking" />
<meta property="og:description" content="Writing is thinking" />
<link rel="canonical" href="https://markpettyjohn.com/jekyll/update/2025/03/06/benchmarks-standardized-testing-ai-education.html" />
<meta property="og:url" content="https://markpettyjohn.com/jekyll/update/2025/03/06/benchmarks-standardized-testing-ai-education.html" />
<meta property="og:site_name" content="Mark Pettyjohn" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-03-06T16:55:42+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="AI Benchmarks: Are We Juking the Stats?" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2025-03-06T16:55:42+00:00","datePublished":"2025-03-06T16:55:42+00:00","description":"Writing is thinking","headline":"AI Benchmarks: Are We Juking the Stats?","mainEntityOfPage":{"@type":"WebPage","@id":"https://markpettyjohn.com/jekyll/update/2025/03/06/benchmarks-standardized-testing-ai-education.html"},"url":"https://markpettyjohn.com/jekyll/update/2025/03/06/benchmarks-standardized-testing-ai-education.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="https://markpettyjohn.com/feed.xml" title="Mark Pettyjohn" />
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-Y0Q6E6Q62N"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-Y0Q6E6Q62N', { 'anonymize_ip': true });
</script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Mark Pettyjohn</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a><a class="page-link" href="/privacy-policy/">Privacy Policy</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">AI Benchmarks: Are We Juking the Stats?</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2025-03-06T16:55:42+00:00" itemprop="datePublished">Mar 6, 2025
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p><img src="/assets/benchmarks.png" alt="Tweet and XKCD reply about needing more benchmarks" /></p>

<p>I think I’ve seen this movie before.</p>

<p>I have experience in education (teaching prek, K, 4th, 6th, and college). I have got experience in tech (90’s AOL/ICQ script kiddie/webmaster turned founder). Today I’m your average generative AI user.</p>

<p>I care about SWE-bench rankings about as much as I cared about standardized test scores as a teacher.</p>

<p>Aside from quickly discerning if a model is at least GPT-4 class, benchmarks don’t tell me anything about how useful a model will be. Very similarly, standardized tests only gave me the broadest sense of a student’s knowledge and capabilities. Even then, tests suffered from false positives. Meaning a student might show proficiency on a test, but if you dig a bit beyond the surface  said student might not actually have learned what the test showed. How does that happen? One way is by teaching to the test.</p>

<p>The call for better benchmarks in generative AI feels very similar to what has happened in education in America over the past 25 years. What we have done is establish a testing-industrial complex. What we haven’t done is meaningfully improve student outcomes.</p>

<p>So with gen AI, sure oneshotting a prompt is cool, but can Claude 3.7 or 03-mini troubleshoot my code without further making a mess of things?</p>

<p>Will GPT 4.5 better rework my writing than Gemini 2.0 Pro Experimental 2-05?</p>

<p>Does Grok have better context to understand Cognitively Guided Instruction to help me think through data collection?</p>

<p>Those are the kinds of things I want the current class of generative AI to help me with. Benchmarks don’t give me any insight those capabilities.</p>

<h3 id="the-allure-and-illusion-of-measurement">The Allure (and Illusion) of Measurement</h3>

<p>The core idea behind both AI benchmarks and standardized tests is understandable. We want to know if AI models are getting “smarter,” just like we want to know if our students are learning.</p>

<p>On its face, this sounds reasonable. However, take a look at 8th grade math and literacy scores in America. They have not improved in a meaningful manner in 60 years. Yes, you read that correctly. When an article comes out about “test scores are down”, look at two things:</p>
<ol>
  <li>The trend line dating back as far as you can see (for NAEP that’s 1973) and</li>
  <li>Where do the averages fall and what does that mean? When you look at both you will see next to no progress, and an average that means the majority of students are going to fail out of or just scrape by the high school math curriculum. Why does even Harvard now have a remedial math class? This is why.</li>
</ol>

<p>It doesn’t have to be this way. Yet we continue to test the ever loving shit out of students at all levels. I remember kindergarten kids crying at the pre-test they had to take the first week of school as a part of the broader testing regime that now exists. I don’t remember making kids cry being a part of my best practices for creating a classroom environment training.</p>

<p>So over multiple generations we have managed to make no real gains, and create a testing-industrial complex that has unintended albeit real negative consequences.</p>

<p>Standardized tests are not just used to measure student learning. By extension, they’re used to measure teacher effectiveness. Phrases like “quality teachers make the difference” and “every student deserves a quality teacher” are hard to dispute. They are good sound bites! Grifters like Vivek are still out there using them today. Give me a few paragraphs and you’ll see why this is pretty stupid.</p>

<p>Testing has not led to quality teachers for every student. What it has led to is a very American thing. Entrenchment of an ineffective system in the name of quality and accountability. Unlike say, the Japanese, who focus on building a better system to achieve quality instead of looking to blame individuals within the system. It all feels very 1970s/1980s U.S. auto manufacturing and shows that we still haven’t learned what Deming tried to America with his <a href="https://deming.org/lessons-from-the-red-bead-experiment-with-dr-deming/">Red Beads simulation</a>.</p>

<p>Bill Gates, a man whose wealth and work need no introduction, bought into this American idea of teacher quality. The genesis I can’t say, but he published an article in 2010 that kicked off an initiative that married test scores and teacher evaluations to come up with teacher ratings so that we could see who the ‘quality’ teachers were. In industry, we would call this performance management. We can fast forward to 2016 and read the RAND report on the $575 million initiative the Gates Foundation launched based on this premise. The result? A resounding, <a href="https://www.rand.org/pubs/research_reports/RR2242.html">expensive failure</a>. Not only did it not improve student outcomes, in many cases, it actively harmed them.</p>

<p><img src="/assets/prequel.jpg" alt="4 Panel Padme Anakin Star Wars meme about Anakin doing something bad" /></p>

<p>The push for “teacher quality” was a strong siren song. So strong that it spread across the country well before the 2016 RAND report dropped. I was in the classroom during this time. It was something I bet anyone who has worked in an organization of size can relate to. Some top-down fuckery that sucks all of the oxygen out of the room, takes a ton of time, and doesn’t overlap much if at all with the real work or improvement you need to do for your job.</p>

<p>Now a person with an iota of experience in other industries or perhaps even someone who has picked up a book and read some history could see parallels that showed how bad of an idea this was before it even began. In 2015 I remember nodding along as I read the  <a href="https://hbr.org/2015/04/reinventing-performance-management">HBR article about performance management at Deloitte</a>. They put some hard numbers to something that everyone seems to know, performance management is a major time suck and divorced from actual performance or employee improvement. At this exact time, education was doubling down on a heavyweight performance management system thanks to Gates and the push for “teacher quality.”</p>

<p>But hey, that was 2015 when things were already in full swing, and maybe Gates doesn’t read HBR.</p>

<p>The one that I truly don’t understand was right under Bill’s nose.</p>

<h3 id="performance-managing-microsoft-off-a-cliff">Performance Managing Microsoft Off A Cliff</h3>

<blockquote>
  <p>“Every current and former Microsoft employee I interviewed – every one – cited stack ranking as the most destructive process inside of Microsoft” - Kurt Eichenwald</p>
</blockquote>

<p>In 2012, Kurt Eichenwald published <a href="https://archive.vanityfair.com/article/2012/8/microsofts-lost-decade">Microsoft’s Lost Decade</a>. This article became a cultural touchstone. Why? Because it pulled back the curtain on how the tech juggernaut could whiff on the two biggest opportunities in tech (mobile and social)since the internet itself and be late to the third (cloud). Gates, who was still chairman throughout the 2000’s had a front row seat to see how Microsoft’s malevolent performance management system and its stack ranking of employees had played a huge role in the company enduring a lost decade.</p>

<p>For anyone who is unfamiliar with stack ranking, imagine you were a CIA agent trying to sabotage a company. You want to make the company slow and distrustful. You want to kill employee morale, motivation, and productivity. You want to make high performers hesitant to work with each other. You want to destroy collaboration. Your primary goal would be to get the company to adopt stack ranking.</p>

<p>This system that turned Gates’ company into an incapable sloth is what he proposed as the antidote for America’s education woes.</p>

<p>Bill Gates must know something I don’t to have thought bringing this type of performance management to education was a good idea after it had ravaged Microsoft. What that is though, I cannot fathom.</p>

<h3 id="goodharts-law-when-the-measure-becomes-the-target">Goodhart’s Law: When the Measure Becomes the Target</h3>

<p>This brings us to Goodhart’s Law. When a measure becomes a target, it ceases to be a good measure.</p>

<p>An example: if you tell a factory manager her variable compensation is tied to a reduction in scrap, then she will encourage her shift managers to push every piece produced out the door. Managers may try to be judicious at first. There will be subjective calls someone has to make. They’ll inevitably green light some questionable pieces. Floor workers will see this. They’ll get looser with their QA checks. They’ll stop asking the manager and just box up every piece. New workers he come in will have no idea it is supposed to be done any other way. This one measure has just helped destroy a culture of quality within the organization. Rejected pieces returned by customers now languish in the warehouse because processing them would hit the factory manager’s scrap numbers. The system gets gamed, the company loses goodwill with customers, problems are pushed down the road, but the manager earns her variable comp.</p>

<p>In education, this plays out tragically. Picture this scenario which, sadly, happens too often in schools:</p>

<ol>
  <li><strong>Low Test Scores:</strong> A school district’s standardized test scores are low.</li>
  <li><strong>State Threatens Takeover:</strong> The state warns of intervention if scores don’t improve within a set timeframe.</li>
  <li><strong>Curriculum Narrows:</strong> Subjects like social studies and science get sidelined. The focus shifts almost entirely to the subjects tested – usually math and reading.</li>
  <li><strong>“Teaching to the Test”:</strong> Instruction becomes laser-focused on the specific content and format of the standardized test.</li>
  <li><strong>Short-Cycle Assessments (SCAs):</strong> Schools create mini-tests that mimic the big standardized test, administering them throughout the year.</li>
  <li><strong>Teacher Evaluations Tied to SCAs:</strong> Teachers’ performance reviews are tied to student performance on these mini-tests.</li>
  <li><strong>Teaching to the Mini-Test:</strong> The cycle intensifies, with instruction now geared towards the mini-tests, which were designed to prepare for the main test.</li>
</ol>

<p>The result?</p>

<p>A miserable experience for students and teachers, a narrowed curriculum, and, often, outright cheating. It’s like that scene from The Wire when Prezbo realizes the school is <a href="https://www.youtube.com/watch?v=_ogxZxu6cjM">“juking the stats”</a>––manipulating the numbers without actually improving the underlying reality, just like the police force did with crime stats when he was a cop.</p>

<p>There are decades of <a href="https://chat.deepseek.com/a/chat/s/165f7e13-09a6-4fda-ab6d-39671ab1ffc2">documented evidence</a> of juking the stats in education. Go back further and look into a different industry, manufacturing, and you’ll see guys like W. Edwards Deming talking about ‘tampering’ for short-term or fleeting improvements instead of doing things that actually improve the system.</p>

<p>And the worst part? These standardized tests, in their current form, are largely useless for improving actual teaching and learning. Analyzing scores months after the fact, when the students have moved on to the next grade, provides little to no actionable information for educators.</p>

<h3 id="formative-assessment-vs-summative-assessment-the-key-difference">Formative Assessment vs. Summative Assessment: The Key Difference</h3>

<p>Here’s where we get to a critical distinction: formative vs. summative assessment.</p>

<ul>
  <li>
    <p><strong>Summative Assessment</strong>: This is the “big” test at the end – the standardized test, the final exam, the AI benchmark. It’s designed to provide a summary of performance. Short-cycle assessments or quarterly mini-tests also fall into this category.</p>
  </li>
  <li>
    <p><strong>Formative Assessment</strong>: This is the ongoing, day-to-day assessment that happens during the learning process. It’s about understanding how students are thinking, identifying misconceptions, and adjusting instruction accordingly.</p>
  </li>
</ul>

<p>As a teacher, formative assessment is my bread and butter. It’s about having a mental model of how students progress in a subject. I concern myself with elementary students building a solid foundation in arithmetic that they can transfer to algebra and higher level mathematics. Number sense to additive reasoning. Making the big conceptual leap to multiplicative reasoning. Building an understanding of properties of operations. Using multiplicative reasoning to understand fractional reasoning, which paves the way to ratios/proportions, and then algebraic reasoning.</p>

<p>With this framework in mind, I’m continuously looking for and eliciting student thinking while teaching. When a student struggles, I have a good idea how to help because I understand this framework and where the student’s reasoning is within it, then what to do about it. With formative assessments I can see, understand, and give students timely, specific feedback.</p>

<p>For example, I have seen 4th grade students solve a four-digit subtraction problem like 4,324 - 749. They use the standard subtraction algorithm and have the correct answer on their paper. However, by observing their work and asking them to explain their process I’ve found that they understand place value up to the tens place, but struggle beyond that. A simple “right” or “wrong” answer on a standardized test would miss this crucial nuance. Teaching without this kind of formative assessment is why too many students end up building a mathematical House of Cards that is bound to fall down.</p>

<h3 id="evals-vs-benchmarks-the-ai-analogy">Evals vs. Benchmarks: The AI Analogy</h3>
<p>Until recently, I had mentally bucketed AI benchmarks and evals together. I had an inkling that evals might be different, but I hadn’t done my homework. Seeing the term <a href="https://news.ycombinator.com/item?id=43246073">‘evals’ used incorrectly</a> like this muddied the waters. Then I listened to <a href="https://www.lennysnewsletter.com/p/why-soft-skills-are-the-future-of-work-karina-nguyen">Karina Nguyen talking to Lenny</a> and evals clicked for me. Evals, like formative assessments, are crucial for progress.</p>

<ul>
  <li><strong>Benchmarks:</strong> These look like the broad, standardized tests designed to compare different models across a range of capabilities.</li>
  <li><strong>Evals:</strong> I think these are more like formative assessments. They’re customizable, task-specific evaluations used to guide the development of a particular model or feature. The kind of thing that is, and should be, a moving target.</li>
</ul>

<h3 id="the-vibe-check-when-intuition-matters">The “Vibe” Check: When Intuition Matters</h3>

<p>François Chollet, put it perfectly: “When a human-facing system becomes sufficiently complex, ‘vibes’ become a perfectly valid evaluation methodology.”</p>

<p>This might sound unscientific, but it resonates deeply with my experience as a teacher. I can often get a good sense of a student’s understanding simply by talking to them and listening to their explanations–even if they don’t get the “right” answer on a formal assessment. A student with a wrong answer may be closer to building a solid conceptual understanding than one who gets the right answer but merely has a procedure memorized.</p>

<p>The same applies to AI. I’ve spent good chunks of time wrestling with some code, getting nowhere with even the latest SOTA models, and realize that, despite the impressive benchmark scores, the “vibes” are off. It just doesn’t work for me in a practical, real-world setting.</p>

<h3 id="the-problem-with-proxies">The Problem with Proxies</h3>

<p>The fundamental issue with AI benchmarks, standardized tests, and even many corporate performance evaluations, is that they rely on proxies. We can’t directly measure intelligence, learning, or the complex value of knowledge work. As in, we truly do not know how to measure these things directly. So we create these stand-ins, these metrics, that we hope will correlate with the real thing.</p>

<p>But these proxies often become distorted. They incentivize narrow optimization, “teaching to the test,” and gaming the system. They create a disconnect between the measured performance and the actual capability. When we forget that these proxies aren’t the real thing we want to measure, we end up with results like we have in education in America. Generations of no meaningful progress.</p>

<h3 id="so-where-does-this-leave-us">So, Where Does This Leave Us?</h3>

<p>The desire for benchmarks in AI feels similar to the education system’s reliance on standardized tests. 
Both are:</p>
<ul>
  <li><strong>Time-Consuming:</strong> They suck up a lot of resources and attention.</li>
  <li><strong>Prone to Perverse Incentives:</strong> They encourage optimizing for the test, not the underlying goal.</li>
  <li><strong>Ultimately, Not That Useful:</strong> They provide a limited, often misleading, picture of actual capability.</li>
</ul>

<p>When generative AI (or whatever comes next) truly reaches a level of general intelligence, we won’t need benchmarks to tell us. We’ll know.</p>

<p>If model providers get sidetracked by teaching to the test and optimizing for benchmarks that would make me bearish for two reasons:</p>
<ol>
  <li>
    <p>They can’t run a tight ship. I’ve never seen a great founder, manager, or teacher focus on teaching to the test (optimizing for the short-term at the expense of the long-term). The model providers who don’t have existing businesses that are cash cows– I’m talking about the Anthropic’s and OpenAI’s, are trying to do something unprecedented. With the goal of AGI/ASI, these companies have to productize the present, while researching and developing the future. That will pull these companies in many directions, which is difficult for even the best run orgs to manage. If one of those directions ends up being benchmark optimization, that would be a waste.</p>
  </li>
  <li>
    <p>If the goal is AGI/ASI, and you’re optimizing for short-term benchmarks, that’s telling me you don’t think you can actually get to the goal.</p>
  </li>
</ol>

<p>In the meantime, as your average AI user, vibe checks are all I need.</p>


  </div><a class="u-url" href="/jekyll/update/2025/03/06/benchmarks-standardized-testing-ai-education.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Mark Pettyjohn</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Mark Pettyjohn</li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Writing is thinking</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
