---
layout: post
title:  "Is Meta Having Its Microsoft Moment?"
date:   2025-04-10 11:55:42 -0500
categories: jekyll update
---

> "Every current and former Microsoft employee I interviewed -- every one -- cited stack ranking as the most destructive process inside of Microsoft" - Kurt Eichenwald

This quote lives rent free in my head. If stack ranking has no haters, I am dead.

I'm a firm believer in the potential of people as well as being fair. It's the decent thing to do and it also makes good business sense. Set clear expectations, ensure people know what they are supposed to do, ensure they know a way to do it, and ensure they receive feedback as to whether they are doing it well or not. Modern performance management does almost none of that. It's not quite an Orwellian term, but it is euphemistic. Performance management is subjectivity masquerading as objectivity. It almost never motivates or actually improves performance, but it is great at demotivating an employee and causing dissatisfaction. 

Layer in some kind of forced ranking or curve system such as stack ranking, or the name du jour for it, and you have taken an already bad system and made it toxic. I don't care what lens you look at it through, it is a horrible, no good, very bad system. Yes, I can strap on my executive hat and look at it from the 30,000 foot view.

The higher order effects of stack ranking can cripple even the most venerable organizations. No thought experiment is needed, because we have Microsoft as evidence. 

Yes, Microsoft is still here today. It is hard to take down an aircraft carrier. But two things can be true at the same time. Stack ranking was disastrous for Microsoft and destroyed its ability to do anything but ride on the back of its legacy systems throughout the 2000's. Microsoft could not innovate, could not ship, and could not coordinate to save its life.

But humans are very good at not learning from history. B.H. Liddell Hart wrote a neat little book about that in 1944. If Hart doesn't jive with you, then maybe Hegel will. The point being, tech CEOs seem to still love stack ranking in spite of its ugly flaws on display. It's like having an affinity for cigarettes in 2025. I don't get it.
https://www.amazon.com/Why-Dont-We-Learn-History/dp/0985081139
https://old.reddit.com/r/askphilosophy/comments/w7gv5q/what_does_hegel_mean_by_the_only_thing_that_we/

I have watched over the years since that Eichenwald piece came out that exposed how stack ranking neutered Microsoft's innovative abilities and seen how stack ranking refuses to die. Silicon Valley is amazing and also full of shit. "Data driven" companies and CEOs are still rolling out their own vesions of forced ranking in spite of the evidence of its ineffectiveness. 
https://www.nysscpa.org/article-content/tech-companies-revive-stack-ranking-even-though-it-has-been-shown-to-be-ineffective-021623

One painful example from the mid-2010's was from Marissa Mayer who took stack ranking to Yahoo! The company was already behind its peers with its main value being derived from its shares in Alibaba when Mayer came on board. For Yahoo!, stack ranking was like taking the head of a drowning person and holding it underwater. Mayer held an infamous all-hands where she read the children's book If Bobbie Had a Nickel to all 4,000 Yahoos who had wanted her to address the disastrous effect stack ranking was having on the company. 
https://www.wwno.org/2015-01-06/can-marissa-mayer-save-yahoo

Meta/Facebook has had an intense evaluation system for quite awhile. Remember, an aircraft carrier is so large and has so much momentum, the sea can't stop it. 



The problem is that humans are a fearful species. Fear is a tremendous motivator at bringing out the worst in our nature. If you want people to stab each other in the back, malicious compliance, 

Performance management has its defenders. One thing I dislike about it is how divorced it is from the work that needs to be done to be effective. Go to a company that has a form of stack ranking and this will be on full display. Employees will respond perfectly to their incentives, which will be to do exactly what they need to do for their performance review. This is so rarely what needs to be done to be effective.

Yes, lots of big co's do this. That doesn't mean it's effective. This is another lesson that must repeatedly be learned. Just because a big company does it, doesn't mean it's an effective practice. How much has Amazon wasted on Alexa? How's the metaverse going? Have you enjoyed talking to Siri lately?

Back to Meta.

Llama 4 dropped on Saturday and it appears, fine. I have zero insider knowledge, but I am a pattern matching machine. When a company has stack ranking I expect it to eventually show signs. Meta is still a massive aircraft carrier. Earnings will continue, they will persist, but internally, I have to wonder if they are experiencing what Microsoft did in the 2000's. 

Instead of a group of people working together towards a common goal, trying to amplify each others' strengths and collectively make their individual weaknesses irrelevant, you have people learning what games they have to play to avoid the culling.

https://www.jointaro.com/question/lScy3l2NPIrWvRuHOjcF/how-does-stack-ranking-work-at-faang-and-how-can-i-be-proactive-at-a-base-level/
https://www.promotions.fyi/company/meta/performance-review

"Was at Amazon. If you’re on a small team of 5 and everyone is a high performer, you have people sabotaging each other. Nobody wants to help because it takes time away from each other. This is a bad move for a shitty culture and you can’t trust anyone - even your boss.

This is a short term gain, long term loss. Amazon has delivered shit for AI and is in last place. Why would you follow their culture?"

Is this anecdote true? I don't know, but this is exactly what stack ranking promotes. It is unbelieveably dumb, but looking at what is happening in America right now, this kind of stupidity seems par for the course.
https://www.reddit.com/r/wallstreetbets/comments/1i1ejp7/meta_is_cutting_5_of_its_lowest_performers/




And also not being stupid. Stack ranking is stupid. It's nerfing your own business. 


When you are an aircraft carrier, very little can knock you off course.




In 2012, Kurt Eichenwald published [Microsoft’s Lost Decade][msft-lost]. This article became a cultural touchstone. Why? Because it pulled back the curtain on how the tech juggernaut could whiff on the two biggest opportunities in tech (mobile and social)since the internet itself and be late to the third (cloud). Gates, who was still chairman throughout the 2000's had a front row seat to see how Microsoft's malevolent performance management system and its stack ranking of employees had played a huge role in the company enduring a lost decade. 

For anyone who is unfamiliar with stack ranking, imagine you were a CIA agent trying to sabotage a company. You want to make the company slow and distrustful. You want to kill employee morale, motivation, and productivity. You want to make high performers hesitant to work with each other. You want to destroy collaboration. Your primary goal would be to get the company to adopt stack ranking. 

This system that turned Gates’ company into an incapable sloth is what he proposed as the antidote for America’s education woes.

### Performance Managing Microsoft Off A Cliff



TL;DR

METR wrote a [paper][ai-agents] (reasonable, measured) and [tweeted][metr-tweet] (sensational) that AI agent autonomy was doubling every seven months, insinuating that there was a new "Moore's Law" for AI agents. You can tell by people's reactions who read the paper and who looked at the tweet. 

There is no new Moore’s Law for AI Agents. 

Read on to see why.

Original tweet that got all of the attention.

![Moore's Law for AI Agents](/assets/METRtweet.png "Tweet from the official METR account that states: In new research, we find a kind of “Moore’s Law for AI agents”: the length of tasks that AIs can do is doubling about every 7 months. The length of tasks AIs can do is doubling about every 7 months.")

More measured tweet from METR’s founder and CEO. Notice the 1500x fewer views than the original.

!["Persnickety title"](/assets/persnick.png "Tweet from Elizabeth Barnes, the founder and CEO of METR that reads: Persnickety title would be: "there's an exponential trend with doubling time between ~2 -12 months on automatically-scoreable, relatively clean + green-field software tasks from a few distributions". More detail on how we thought about external validity in paper and this thread")

### Moore’s Law—Predictable from Observation. Predictive from Roadmap.

> “The number of transistors in an integrated circuit doubles every two years.”
- Gordon Moore

Gordon Moore's infamous prediction was made [in 1965 after observing two years of this growth][moores-law]. It ended up being predictive because it was based on four well-understood engineering challenges with clear paths for improvement.

1. Miniaturizing transistors and wires, fitting more onto a chip.
2. Improving the printing process for chips with four known ways to improve photolithography.
3. Making the wafers larger to accommodate more transistors.
4. Improving circuit design.

Moore made the projection based on the observed trends in a rapidly developing, but fundamentally understandable technology. 

Now, did this prediction become a target that motivated the industry? Yes. History shows it created enormous pressure across the industry to invest in R&D to drive this technological progress.

The observation and the pressure are where similarities between Moore and METR end.




### METR’s Law for AI Agents?
No. 

In contrast with Moore’s Law there is not a set of well-understood engineering challenges with clear paths for improvement that will continue to drive growth in AI agents’ abilities.

The measurements for the engineering challenges Moore saw were objective. METR's attempt to quantify AI Agent ability is subjective. If you read the research paper as well as the [HCAST, Human Calibrated Autonomy Software Tasks][hcast] paper that it references you'll see how much subjectivity went into coming up with a way to measure AI agent ability.

That's a longer way of saying, microchip measurements were straightforward. AI agent ability is measuring knowledge work, which is fundamentally subjective.

#### The Contrast With Moore's Law

In generative AI, METR is observing an early trend, similar to Moore. 

But what’s going to continue driving improvements that could enable doubling?

Who, if anyone, truly understands the underlying technology? There is a lot going on underneath the hood of a generative AI model. [Anthropic’s research][anthropic-thoughts] about understanding the thoughts of a language model demonstrates both how much is happening and how much we don’t yet understand about this technology.

### An AI roadmap or Premature Extrapolation

Capital has poured into GenAI and continues to do so. The large labs are in constant competition. The pressure that Gordon Moore thought was needed to drive Moore’s Law already exists in spades in the generative AI industry.

It’s the roadmap that is missing. 

What’s the data, algorithm, compute, or unknown that’s going to get us to autonomy? 

The waters are muddy. 

The rhetoric from Sam and Dario has been off the charts about expecting something approximating AGI. They have better seats than anyone to see the whole field, but we also have to discount what they say. For one, CEOs will always talk their book. Second, we’ve seen how some of what they have proclaimed hasn’t proven true (e.g. scaling laws for pre-training will drive us to AGI.)

I don’t see an equivalent to Gordon Moore’s four-part roadmap. Instead I see unknowns. Does more basic research need to be done? Could we strip mine the mountain of AI research that already exists for ideas that could unlock potential breakthroughs that could make METRs observation into a continued reality?

The tsunami of cash being poured into the field alongside a history of AI that stretches back 70 years makes for ripe conditions for advances. What those may be though are unclear.

### Communication Is Perception
Here's one I'm torn on, but it seems worth calling out.

Is METR's headline what we need communicated right now?

Communication is perception. 

It doesn’t matter what you say, it matters what others hear. This is a principle often ignored by people that try to communicate.

What people are hearing in reaction to this headline is “AI will take over knowledge work.”

[Andrew Yang’s reaction][yang-reaction] was illustrative and representative of reactions to METR's announcement.

There’s an argument that METR’s research puts pressure on the world to become aware of the potential for a fundamental reordering of society.

There’s also an argument that this is alarmist and untrue.

The U.S. is alight in gaslighting these days and maybe I'm just irked at a sensational headline. 

But at the same time, for someone whose life's work is to build a better education system, the potential of AI to reorder society as we know has some pretty big implications for that work. 

Trying to sort through that while in the fog of uncertain and unclear times is a challenge in its own right, and misleading headlines don't help.

[ai-agents]: https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/
[metr-tweet]: https://x.com/METR_Evals/status/1902384481111322929
[hcast]: https://metr.org/hcast.pdf
[moores-law]: https://hasler.ece.gatech.edu/Published_papers/Technology_overview/gordon_moore_1965_article.pdf
[anthropic-thoughts]: https://www.anthropic.com/news/tracing-thoughts-language-model
[yang-reaction]: https://x.com/AndrewYang/status/1902468574641328417