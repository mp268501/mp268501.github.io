<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://markpettyjohn.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://markpettyjohn.com/" rel="alternate" type="text/html" /><updated>2025-08-05T20:21:23+00:00</updated><id>https://markpettyjohn.com/feed.xml</id><title type="html">Mark Pettyjohn</title><subtitle>Writing is thinking</subtitle><entry><title type="html">What happened with OpenAI’s offer to buy Windsurf?</title><link href="https://markpettyjohn.com/jekyll/update/2025/07/15/my-windsurf-question.html" rel="alternate" type="text/html" title="What happened with OpenAI’s offer to buy Windsurf?" /><published>2025-07-15T16:55:42+00:00</published><updated>2025-07-15T16:55:42+00:00</updated><id>https://markpettyjohn.com/jekyll/update/2025/07/15/my-windsurf-question</id><content type="html" xml:base="https://markpettyjohn.com/jekyll/update/2025/07/15/my-windsurf-question.html"><![CDATA[<p>Windsurf turned down $3 billion from OpenAI to take $2.4 billion from Google?</p>

<p>Something is off, and I haven’t seen it addressed yet.</p>

<p>I like history because it helps me understand human nature. Human nature doesn’t change. I value <a href="https://markpettyjohn.com/jekyll/update/2025/03/11/know-the-game-you-play.html">knowing the game you’re playing</a>.  Having a fleshed out understanding of human nature is a cheat code for understanding games that involve people.</p>

<p>With the Windsurf saga I don’t understand the game.</p>

<p>Some facts: OpenAI extended a $3 billion offer to purchase Windsurf. The stated sticking point was not wanting to share Windsurf IP with Microsoft. Who didn’t want to share the IP? According to Bloomberg Windsurf was the party that didn’t want Microsoft to have access to its IP. That puts Varun and maybe Douglas Chen in the driver’s seat for this stipulation.</p>

<blockquote>
  <p>OpenAI thought it had an agreement with Windsurf and almost announced the acquisition in early May, according to people familiar with the deal. There had been a signed letter of intent, and Windsurf investors were given what’s known as waterfall agreements, notifying each party how much money they were expected to make, the people said.</p>
</blockquote>

<p>It seems like this arrangement would have seen Varun, Douglas Chen, and the entirety of the Windsurf team go to OpenAI.</p>

<p>Instead, Varun and Chen took only a team of researchers to Deepmind at a $600 million discount. That sticking point about sharing IP with Microsoft? Apparently not an issue anymore as big G received a non-exclusive license to use Windsurf’s IP.</p>

<p>Something doesn’t add up for me here. Founders and VCs leaving $600 million on the table while creating a confusing deal that paints the founders in a bad light?</p>

<p>As the smoke clears, I’ll be curious to see what really went down and then add that to my mental model about the game that’s being played in the wild wild west of the new AI world.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[Windsurf turned down $3 billion from OpenAI to take $2.4 billion from Google?]]></summary></entry><entry><title type="html">No. It’s Not A New Moore’s Law for AI Agents</title><link href="https://markpettyjohn.com/jekyll/update/2025/03/29/not-moores-law-for-ai-agents.html" rel="alternate" type="text/html" title="No. It’s Not A New Moore’s Law for AI Agents" /><published>2025-03-29T16:55:42+00:00</published><updated>2025-03-29T16:55:42+00:00</updated><id>https://markpettyjohn.com/jekyll/update/2025/03/29/not-moores-law-for-ai-agents</id><content type="html" xml:base="https://markpettyjohn.com/jekyll/update/2025/03/29/not-moores-law-for-ai-agents.html"><![CDATA[<p>TL;DR</p>

<p>METR wrote a <a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">paper</a> (reasonable, measured) and <a href="https://x.com/METR_Evals/status/1902384481111322929">tweeted</a> (sensational) that AI agent autonomy was doubling every seven months, insinuating that there was a new “Moore’s Law” for AI agents. You can tell by people’s reactions who read the paper and who looked at the tweet.</p>

<p>There is no new Moore’s Law for AI Agents.</p>

<p>Read on to see why.</p>

<p>Original tweet that got all of the attention.</p>

<p><img src="/assets/METRtweet.png" alt="Moore's Law for AI Agents" title="Tweet from the official METR account that states: In new research, we find a kind of “Moore’s Law for AI agents”: the length of tasks that AIs can do is doubling about every 7 months. The length of tasks AIs can do is doubling about every 7 months." /></p>

<p>More measured tweet from METR’s founder and CEO. Notice the 1500x fewer views than the original.</p>

<p><img src="/assets/persnick.png" alt="&quot;Persnickety title&quot;" title="Tweet from Elizabeth Barnes, the founder and CEO of METR that reads: Persnickety title would be: &quot;there's an exponential trend with doubling time between ~2 -12 months on automatically-scoreable, relatively clean + green-field software tasks from a few distributions&quot;. More detail on how we thought about external validity in paper and this thread" /></p>

<h3 id="moores-lawpredictable-from-observation-predictive-from-roadmap">Moore’s Law—Predictable from Observation. Predictive from Roadmap.</h3>

<blockquote>
  <p>“The number of transistors in an integrated circuit doubles every two years.”</p>
  <ul>
    <li>Gordon Moore</li>
  </ul>
</blockquote>

<p>Gordon Moore’s infamous prediction was made <a href="https://hasler.ece.gatech.edu/Published_papers/Technology_overview/gordon_moore_1965_article.pdf">in 1965 after observing two years of this growth</a>. It ended up being predictive because it was based on four well-understood engineering challenges with clear paths for improvement.</p>

<ol>
  <li>Miniaturizing transistors and wires, fitting more onto a chip.</li>
  <li>Improving the printing process for chips with four known ways to improve photolithography.</li>
  <li>Making the wafers larger to accommodate more transistors.</li>
  <li>Improving circuit design.</li>
</ol>

<p>Moore made the projection based on the observed trends in a rapidly developing, but fundamentally understandable technology.</p>

<p>Now, did this prediction become a target that motivated the industry? Yes. History shows it created enormous pressure across the industry to invest in R&amp;D to drive this technological progress.</p>

<p>The observation and the pressure are where similarities between Moore and METR end.</p>

<h3 id="metrs-law-for-ai-agents">METR’s Law for AI Agents?</h3>
<p>No.</p>

<p>In contrast with Moore’s Law there is not a set of well-understood engineering challenges with clear paths for improvement that will continue to drive growth in AI agents’ abilities.</p>

<p>The measurements for the engineering challenges Moore saw were objective. METR’s attempt to quantify AI Agent ability is subjective. If you read the research paper as well as the <a href="https://metr.org/hcast.pdf">HCAST, Human Calibrated Autonomy Software Tasks</a> paper that it references you’ll see how much subjectivity went into coming up with a way to measure AI agent ability.</p>

<p>That’s a longer way of saying, microchip measurements were straightforward. AI agent ability is measuring knowledge work, which is fundamentally subjective.</p>

<h4 id="the-contrast-with-moores-law">The Contrast With Moore’s Law</h4>

<p>In generative AI, METR is observing an early trend, similar to Moore.</p>

<p>But what’s going to continue driving improvements that could enable doubling?</p>

<p>Who, if anyone, truly understands the underlying technology? There is a lot going on underneath the hood of a generative AI model. <a href="https://www.anthropic.com/news/tracing-thoughts-language-model">Anthropic’s research</a> about understanding the thoughts of a language model demonstrates both how much is happening and how much we don’t yet understand about this technology.</p>

<h3 id="an-ai-roadmap-or-premature-extrapolation">An AI roadmap or Premature Extrapolation</h3>

<p>Capital has poured into GenAI and continues to do so. The large labs are in constant competition. The pressure that Gordon Moore thought was needed to drive Moore’s Law already exists in spades in the generative AI industry.</p>

<p>It’s the roadmap that is missing.</p>

<p>What’s the data, algorithm, compute, or unknown that’s going to get us to autonomy?</p>

<p>The waters are muddy.</p>

<p>The rhetoric from Sam and Dario has been off the charts about expecting something approximating AGI. They have better seats than anyone to see the whole field, but we also have to discount what they say. For one, CEOs will always talk their book. Second, we’ve seen how some of what they have proclaimed hasn’t proven true (e.g. scaling laws for pre-training will drive us to AGI.)</p>

<p>I don’t see an equivalent to Gordon Moore’s four-part roadmap. Instead I see unknowns. Does more basic research need to be done? Could we strip mine the mountain of AI research that already exists for ideas that could unlock potential breakthroughs that could make METRs observation into a continued reality?</p>

<p>The tsunami of cash being poured into the field alongside a history of AI that stretches back 70 years makes for ripe conditions for advances. What those may be though are unclear.</p>

<h3 id="communication-is-perception">Communication Is Perception</h3>
<p>Here’s one I’m torn on, but it seems worth calling out.</p>

<p>Is METR’s headline what we need communicated right now?</p>

<p>Communication is perception.</p>

<p>It doesn’t matter what you say, it matters what others hear. This is a principle often ignored by people that try to communicate.</p>

<p>What people are hearing in reaction to this headline is “AI will take over knowledge work.”</p>

<p><a href="https://x.com/AndrewYang/status/1902468574641328417">Andrew Yang’s reaction</a> was illustrative and representative of reactions to METR’s announcement.</p>

<p>There’s an argument that METR’s research puts pressure on the world to become aware of the potential for a fundamental reordering of society.</p>

<p>There’s also an argument that this is alarmist and untrue.</p>

<p>The U.S. is alight in gaslighting these days and maybe I’m just irked at a sensational headline.</p>

<p>But at the same time, for someone whose life’s work is to build a better education system, the potential of AI to reorder society as we know has some pretty big implications for that work.</p>

<p>Trying to sort through that while in the fog of uncertain and unclear times is a challenge in its own right, and misleading headlines don’t help.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[TL;DR]]></summary></entry><entry><title type="html">Cognitive Dissonance</title><link href="https://markpettyjohn.com/jekyll/update/2025/03/26/cognitive-dissonance.html" rel="alternate" type="text/html" title="Cognitive Dissonance" /><published>2025-03-26T16:55:42+00:00</published><updated>2025-03-26T16:55:42+00:00</updated><id>https://markpettyjohn.com/jekyll/update/2025/03/26/cognitive-dissonance</id><content type="html" xml:base="https://markpettyjohn.com/jekyll/update/2025/03/26/cognitive-dissonance.html"><![CDATA[<p>I bumped into an insidious and powerful force again this week. Something that flexes its muscle behind the scenes–that sneaky little SOB, cognitive dissonance.</p>

<p>My habits brought this to the forefront this week. I deeply recognize their importance. They create a positive flywheel effect. When that positive flywheel is spinning I feel good, I am healthier, think more clearly, create more, am more enjoyable to be around, and even my consumption habits become more mindful and enjoyable. I want that.</p>

<p>However, this positive flywheel is built with too many easy off-ramps. One slip leads to another, leads to another, and then I’m zipping around on a negative flywheel without really being conscious of it.</p>

<p>At this stage of my life, I have a significant degree of freedom to choose how I spend my time. It’s like living in a massively open-world game. But that freedom can be a double-edged sword. A weekend trip disrupts routine. Then I don’t eat any regular meals on Monday. A late night working leads to tiredness on Tuesday. Tiredness invites snacking and sweets. Cognition nosedives. Suddenly, without any deliberate decision, I’m spinning downward on that negative flywheel.</p>

<p>I understand these Dark Arts on an intellectual level. I have tactics and practice with defense against them, but they’re not called the Dark Arts for no reason. They know how to short-circuit those defenses.</p>

<p>Ok, so those are habits, but here is where the cognitive dissonance comes into play.</p>

<p>It hit me while thinking about a friend’s toddler, who is grumpy during routine things they do every day. I’ve taught preschoolers and kindergartners. I know that young children rely on adults to be their prefrontal cortex. Routines and habits are essential. Stick to them, and daily activities like meals and bedtime are manageable. Deviate and you’re setting yourself up for tantrums. Structure helps keep our little friends regulated.</p>

<p>I don’t offer unsolicited advice (it’s rarely welcomed or acted upon), but as I watch my friends struggle, I can clearly see how implementing some simple regularity could alleviate so much stress for them.</p>

<p>Mind you, at the same time I’m observing how a little more regularity would go a long way in making things easier for this family, my adult brain is not only failing to provide the structure I know I need, it’s got me thinking we’re cruising along just fine. The warning lights are flashing on the dashboard of my car yet I don’t even notice them two feet from my face.</p>

<p>Pot, meet kettle.</p>

<p>Richard Feynman famously <a href="https://calteches.library.caltech.edu/51/2/CargoCult.htm">stated at a Caltech commencement that</a>, “The first principle is that you must not fool yourself—and you are the easiest person to fool.”</p>

<p>I’ve built part of my career on my ability to observe how executives and organizations fool themselves. And yet, when it comes to looking in the mirror, I can still pull the wool over my own eyes.</p>

<p>The remedy though is simple.</p>

<p>Reflection.</p>

<p>There are plenty of tools and tactics to reflect: walks, voice memos, Morning Pages, friends, coaches, therapists or even talking to an LLM.</p>

<p>The key thing to remember though is that if fooling ourselves is a first principle of human nature, then reflection, is a first principle of seeing reality.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[I bumped into an insidious and powerful force again this week. Something that flexes its muscle behind the scenes–that sneaky little SOB, cognitive dissonance.]]></summary></entry><entry><title type="html">Know the Game You’re Playing</title><link href="https://markpettyjohn.com/jekyll/update/2025/03/11/know-the-game-you-play.html" rel="alternate" type="text/html" title="Know the Game You’re Playing" /><published>2025-03-11T16:55:42+00:00</published><updated>2025-03-11T16:55:42+00:00</updated><id>https://markpettyjohn.com/jekyll/update/2025/03/11/know-the-game-you-play</id><content type="html" xml:base="https://markpettyjohn.com/jekyll/update/2025/03/11/know-the-game-you-play.html"><![CDATA[<blockquote>
  <p>“It’s remarkable how much you don’t know about the game you’ve been playing all of your life.”</p>
  <ul>
    <li>Mickey Mantle</li>
  </ul>
</blockquote>

<p>There are games everywhere in life. In America there is a master game that we all have to play in. Its effects are pervasive and regularly visible. Yet to many people, the effects are misunderstood or misattributed.</p>

<p>The game is capitalism. A form of capitalism with a very specific set of rules that were set out and codified in the late twentieth century. There’s regular evidence showing how unaware people are to the game they’ve been playing all of their lives. Fortunately, it’s a simple game to understand.</p>

<p>The latest example comes from Southwest Airlines’ decision to get rid of “bags fly free.” This comes on the heels of eliminating open seating earlier in the year. Corporate greed. Out of touch executives. MBA creep. None of these reactions people have had are wrong, but they’re misattributions. They don’t understand the rules of the game.</p>

<p>The game is Milton Friedman’s capitalism. It’s a paradigm that took hold in mindshare, judicial thinking, and legal interpretation in the second half of the twentieth century.</p>

<h3 id="what-are-the-rules">What are the rules?</h3>

<p>People running a public company have one legal responsibility, and that responsibility is a fiduciary duty to shareholders. Not to customers, not to the community, not to a country.  Fiduciary is a fancy way of saying trust. That could have a broad interpretation, but in the game of Milton Friedman’s capitalism, fiduciary responsibility has come to mean one thing–driving up the stock price. Stock prices live on a spectrum of numerically driven reality to socially constructed reality. Tesla’s stock price is an example of a socially constructed reality, but it is still based on the story of future growth.</p>

<p>For a company three things can grow:</p>
<ol>
  <li>Revenue (the total amount of everything sold)</li>
  <li>Income (what is left after paying all expenses)</li>
  <li>The story (of future revenue and profits)</li>
</ol>

<p>If you understand these rules, you can make sense of and even anticipate how public companies will behave.</p>

<p>Every year, every quarter public companies must show growth–or sell the public markets on a story of future growth. This understanding is the disconnect people have when they see something like company x has record profits, yet lays off thousands of people. There is a never ending, relentless need to show growth by increasing revenue or cutting costs.</p>

<p>Storytelling is why you hear Daniel Ek brand Spotify as “the future of audio” or Mary Barra call General Motors the future of “zero crashes, zero emissions, and zero congestion”. Why doesn’t Ek just call Spotify a great streaming music service or Barra call GM an automaker? Because they have to sell a story of a bigger future with greater growth.</p>

<p>And yes, that can often result in companies doing things that work to increase growth in the short-term yet are harmful in the long-term. That’s what people are catching on to with memes about private equity ruining everything, and <a href="https://en.wikipedia.org/wiki/Enshittification">‘enshittification’</a> becoming a known concept. Time will tell if that will be Southwest’s fate.</p>

<p>The root though, is the game of Milton Friedman’s capitalism.</p>

<p>There are countless other games we all play every day, whether we want to or not.</p>

<p>Most people cannot opt out. A very select few can change the game, or change the rules. A society is a system after all, and Donella Meadows explained to us that <a href="https://donellameadows.org/archives/leverage-points-places-to-intervene-in-a-system/">the most powerful and lasting way to change a system</a> is a paradigm shift. It’s also the most challenging to do.</p>

<p>Footnote:</p>

<p>This biography about <a href="https://www.goodreads.com/book/show/22673027-king-icahn">Carl Icahn</a> is a good look into the old world being blindsided by the new realities of Milton Friedman’s capitalism. In the 1980’s Icahn ran through companies like a buzzsaw whose managements had not yet realized that the game had changed. These battles defined what “fiduciary responsibility to shareholders” would look like moving forward.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[“It’s remarkable how much you don’t know about the game you’ve been playing all of your life.” Mickey Mantle]]></summary></entry><entry><title type="html">AI Benchmarks: Are We Juking the Stats?</title><link href="https://markpettyjohn.com/jekyll/update/2025/03/06/benchmarks-standardized-testing-ai-education.html" rel="alternate" type="text/html" title="AI Benchmarks: Are We Juking the Stats?" /><published>2025-03-06T16:55:42+00:00</published><updated>2025-03-06T16:55:42+00:00</updated><id>https://markpettyjohn.com/jekyll/update/2025/03/06/benchmarks-standardized-testing-ai-education</id><content type="html" xml:base="https://markpettyjohn.com/jekyll/update/2025/03/06/benchmarks-standardized-testing-ai-education.html"><![CDATA[<p><img src="/assets/benchmarks.png" alt="Tweet and XKCD reply about needing more benchmarks" /></p>

<p>I think I’ve seen this movie before.</p>

<p>I have experience in education (teaching prek, K, 4th, 6th, and college). I have got experience in tech (90’s AOL/ICQ script kiddie/webmaster turned founder). Today I’m your average generative AI user.</p>

<p>I care about SWE-bench rankings about as much as I cared about standardized test scores as a teacher.</p>

<p>Aside from quickly discerning if a model is at least GPT-4 class, benchmarks don’t tell me anything about how useful a model will be. Very similarly, standardized tests only gave me the broadest sense of a student’s knowledge and capabilities. Even then, tests suffered from false positives. Meaning a student might show proficiency on a test, but if you dig a bit beyond the surface  said student might not actually have learned what the test showed. How does that happen? One way is by teaching to the test.</p>

<p>The call for better benchmarks in generative AI feels very similar to what has happened in education in America over the past 25 years. What we have done is establish a testing-industrial complex. What we haven’t done is meaningfully improve student outcomes.</p>

<p>So with gen AI, sure oneshotting a prompt is cool, but can Claude 3.7 or 03-mini troubleshoot my code without further making a mess of things?</p>

<p>Will GPT 4.5 better rework my writing than Gemini 2.0 Pro Experimental 2-05?</p>

<p>Does Grok have better context to understand Cognitively Guided Instruction to help me think through data collection?</p>

<p>Those are the kinds of things I want the current class of generative AI to help me with. Benchmarks don’t give me any insight those capabilities.</p>

<h3 id="the-allure-and-illusion-of-measurement">The Allure (and Illusion) of Measurement</h3>

<p>The core idea behind both AI benchmarks and standardized tests is understandable. We want to know if AI models are getting “smarter,” just like we want to know if our students are learning.</p>

<p>On its face, this sounds reasonable. However, take a look at 8th grade math and literacy scores in America. They have not improved in a meaningful manner in 60 years. Yes, you read that correctly. When an article comes out about “test scores are down”, look at two things:</p>
<ol>
  <li>The trend line dating back as far as you can see (for NAEP that’s 1973) and</li>
  <li>Where do the averages fall and what does that mean? When you look at both you will see next to no progress, and an average that means the majority of students are going to fail out of or just scrape by the high school math curriculum. Why does even Harvard now have a remedial math class? This is why.</li>
</ol>

<p>It doesn’t have to be this way. Yet we continue to test the ever loving shit out of students at all levels. I remember kindergarten kids crying at the pre-test they had to take the first week of school as a part of the broader testing regime that now exists. I don’t remember making kids cry being a part of my best practices for creating a classroom environment training.</p>

<p>So over multiple generations we have managed to make no real gains, and create a testing-industrial complex that has unintended albeit real negative consequences.</p>

<p>Standardized tests are not just used to measure student learning. By extension, they’re used to measure teacher effectiveness. Phrases like “quality teachers make the difference” and “every student deserves a quality teacher” are hard to dispute. They are good sound bites! Grifters like Vivek are still out there using them today. Give me a few paragraphs and you’ll see why this is pretty stupid.</p>

<p>Testing has not led to quality teachers for every student. What it has led to is a very American thing. Entrenchment of an ineffective system in the name of quality and accountability. Unlike say, the Japanese, who focus on building a better system to achieve quality instead of looking to blame individuals within the system. It all feels very 1970s/1980s U.S. auto manufacturing and shows that we still haven’t learned what Deming tried to America with his <a href="https://deming.org/lessons-from-the-red-bead-experiment-with-dr-deming/">Red Beads simulation</a>.</p>

<p>Bill Gates, a man whose wealth and work need no introduction, bought into this American idea of teacher quality. The genesis I can’t say, but he published an article in 2010 that kicked off an initiative that married test scores and teacher evaluations to come up with teacher ratings so that we could see who the ‘quality’ teachers were. In industry, we would call this performance management. We can fast forward to 2016 and read the RAND report on the $575 million initiative the Gates Foundation launched based on this premise. The result? A resounding, <a href="https://www.rand.org/pubs/research_reports/RR2242.html">expensive failure</a>. Not only did it not improve student outcomes, in many cases, it actively harmed them.</p>

<p><img src="/assets/prequel.jpg" alt="4 Panel Padme Anakin Star Wars meme about Anakin doing something bad" /></p>

<p>The push for “teacher quality” was a strong siren song. So strong that it spread across the country well before the 2016 RAND report dropped. I was in the classroom during this time. It was something I bet anyone who has worked in an organization of size can relate to. Some top-down fuckery that sucks all of the oxygen out of the room, takes a ton of time, and doesn’t overlap much if at all with the real work or improvement you need to do for your job.</p>

<p>Now a person with an iota of experience in other industries or perhaps even someone who has picked up a book and read some history could see parallels that showed how bad of an idea this was before it even began. In 2015 I remember nodding along as I read the  <a href="https://hbr.org/2015/04/reinventing-performance-management">HBR article about performance management at Deloitte</a>. They put some hard numbers to something that everyone seems to know, performance management is a major time suck and divorced from actual performance or employee improvement. At this exact time, education was doubling down on a heavyweight performance management system thanks to Gates and the push for “teacher quality.”</p>

<p>But hey, that was 2015 when things were already in full swing, and maybe Gates doesn’t read HBR.</p>

<p>The one that I truly don’t understand was right under Bill’s nose.</p>

<h3 id="performance-managing-microsoft-off-a-cliff">Performance Managing Microsoft Off A Cliff</h3>

<blockquote>
  <p>“Every current and former Microsoft employee I interviewed – every one – cited stack ranking as the most destructive process inside of Microsoft” - Kurt Eichenwald</p>
</blockquote>

<p>In 2012, Kurt Eichenwald published <a href="https://archive.vanityfair.com/article/2012/8/microsofts-lost-decade">Microsoft’s Lost Decade</a>. This article became a cultural touchstone. Why? Because it pulled back the curtain on how the tech juggernaut could whiff on the two biggest opportunities in tech (mobile and social)since the internet itself and be late to the third (cloud). Gates, who was still chairman throughout the 2000’s had a front row seat to see how Microsoft’s malevolent performance management system and its stack ranking of employees had played a huge role in the company enduring a lost decade.</p>

<p>For anyone who is unfamiliar with stack ranking, imagine you were a CIA agent trying to sabotage a company. You want to make the company slow and distrustful. You want to kill employee morale, motivation, and productivity. You want to make high performers hesitant to work with each other. You want to destroy collaboration. Your primary goal would be to get the company to adopt stack ranking.</p>

<p>This system that turned Gates’ company into an incapable sloth is what he proposed as the antidote for America’s education woes.</p>

<p>Bill Gates must know something I don’t to have thought bringing this type of performance management to education was a good idea after it had ravaged Microsoft. What that is though, I cannot fathom.</p>

<h3 id="goodharts-law-when-the-measure-becomes-the-target">Goodhart’s Law: When the Measure Becomes the Target</h3>

<p>This brings us to Goodhart’s Law. When a measure becomes a target, it ceases to be a good measure.</p>

<p>An example: if you tell a factory manager her variable compensation is tied to a reduction in scrap, then she will encourage her shift managers to push every piece produced out the door. Managers may try to be judicious at first. There will be subjective calls someone has to make. They’ll inevitably green light some questionable pieces. Floor workers will see this. They’ll get looser with their QA checks. They’ll stop asking the manager and just box up every piece. New workers he come in will have no idea it is supposed to be done any other way. This one measure has just helped destroy a culture of quality within the organization. Rejected pieces returned by customers now languish in the warehouse because processing them would hit the factory manager’s scrap numbers. The system gets gamed, the company loses goodwill with customers, problems are pushed down the road, but the manager earns her variable comp.</p>

<p>In education, this plays out tragically. Picture this scenario which, sadly, happens too often in schools:</p>

<ol>
  <li><strong>Low Test Scores:</strong> A school district’s standardized test scores are low.</li>
  <li><strong>State Threatens Takeover:</strong> The state warns of intervention if scores don’t improve within a set timeframe.</li>
  <li><strong>Curriculum Narrows:</strong> Subjects like social studies and science get sidelined. The focus shifts almost entirely to the subjects tested – usually math and reading.</li>
  <li><strong>“Teaching to the Test”:</strong> Instruction becomes laser-focused on the specific content and format of the standardized test.</li>
  <li><strong>Short-Cycle Assessments (SCAs):</strong> Schools create mini-tests that mimic the big standardized test, administering them throughout the year.</li>
  <li><strong>Teacher Evaluations Tied to SCAs:</strong> Teachers’ performance reviews are tied to student performance on these mini-tests.</li>
  <li><strong>Teaching to the Mini-Test:</strong> The cycle intensifies, with instruction now geared towards the mini-tests, which were designed to prepare for the main test.</li>
</ol>

<p>The result?</p>

<p>A miserable experience for students and teachers, a narrowed curriculum, and, often, outright cheating. It’s like that scene from The Wire when Prezbo realizes the school is <a href="https://www.youtube.com/watch?v=_ogxZxu6cjM">“juking the stats”</a>––manipulating the numbers without actually improving the underlying reality, just like the police force did with crime stats when he was a cop.</p>

<p>There are decades of <a href="https://chat.deepseek.com/a/chat/s/165f7e13-09a6-4fda-ab6d-39671ab1ffc2">documented evidence</a> of juking the stats in education. Go back further and look into a different industry, manufacturing, and you’ll see guys like W. Edwards Deming talking about ‘tampering’ for short-term or fleeting improvements instead of doing things that actually improve the system.</p>

<p>And the worst part? These standardized tests, in their current form, are largely useless for improving actual teaching and learning. Analyzing scores months after the fact, when the students have moved on to the next grade, provides little to no actionable information for educators.</p>

<h3 id="formative-assessment-vs-summative-assessment-the-key-difference">Formative Assessment vs. Summative Assessment: The Key Difference</h3>

<p>Here’s where we get to a critical distinction: formative vs. summative assessment.</p>

<ul>
  <li>
    <p><strong>Summative Assessment</strong>: This is the “big” test at the end – the standardized test, the final exam, the AI benchmark. It’s designed to provide a summary of performance. Short-cycle assessments or quarterly mini-tests also fall into this category.</p>
  </li>
  <li>
    <p><strong>Formative Assessment</strong>: This is the ongoing, day-to-day assessment that happens during the learning process. It’s about understanding how students are thinking, identifying misconceptions, and adjusting instruction accordingly.</p>
  </li>
</ul>

<p>As a teacher, formative assessment is my bread and butter. It’s about having a mental model of how students progress in a subject. I concern myself with elementary students building a solid foundation in arithmetic that they can transfer to algebra and higher level mathematics. Number sense to additive reasoning. Making the big conceptual leap to multiplicative reasoning. Building an understanding of properties of operations. Using multiplicative reasoning to understand fractional reasoning, which paves the way to ratios/proportions, and then algebraic reasoning.</p>

<p>With this framework in mind, I’m continuously looking for and eliciting student thinking while teaching. When a student struggles, I have a good idea how to help because I understand this framework and where the student’s reasoning is within it, then what to do about it. With formative assessments I can see, understand, and give students timely, specific feedback.</p>

<p>For example, I have seen 4th grade students solve a four-digit subtraction problem like 4,324 - 749. They use the standard subtraction algorithm and have the correct answer on their paper. However, by observing their work and asking them to explain their process I’ve found that they understand place value up to the tens place, but struggle beyond that. A simple “right” or “wrong” answer on a standardized test would miss this crucial nuance. Teaching without this kind of formative assessment is why too many students end up building a mathematical House of Cards that is bound to fall down.</p>

<h3 id="evals-vs-benchmarks-the-ai-analogy">Evals vs. Benchmarks: The AI Analogy</h3>
<p>Until recently, I had mentally bucketed AI benchmarks and evals together. I had an inkling that evals might be different, but I hadn’t done my homework. Seeing the term <a href="https://news.ycombinator.com/item?id=43246073">‘evals’ used incorrectly</a> like this muddied the waters. Then I listened to <a href="https://www.lennysnewsletter.com/p/why-soft-skills-are-the-future-of-work-karina-nguyen">Karina Nguyen talking to Lenny</a> and evals clicked for me. Evals, like formative assessments, are crucial for progress.</p>

<ul>
  <li><strong>Benchmarks:</strong> These look like the broad, standardized tests designed to compare different models across a range of capabilities.</li>
  <li><strong>Evals:</strong> I think these are more like formative assessments. They’re customizable, task-specific evaluations used to guide the development of a particular model or feature. The kind of thing that is, and should be, a moving target.</li>
</ul>

<h3 id="the-vibe-check-when-intuition-matters">The “Vibe” Check: When Intuition Matters</h3>

<p>François Chollet, put it perfectly: “When a human-facing system becomes sufficiently complex, ‘vibes’ become a perfectly valid evaluation methodology.”</p>

<p>This might sound unscientific, but it resonates deeply with my experience as a teacher. I can often get a good sense of a student’s understanding simply by talking to them and listening to their explanations–even if they don’t get the “right” answer on a formal assessment. A student with a wrong answer may be closer to building a solid conceptual understanding than one who gets the right answer but merely has a procedure memorized.</p>

<p>The same applies to AI. I’ve spent good chunks of time wrestling with some code, getting nowhere with even the latest SOTA models, and realize that, despite the impressive benchmark scores, the “vibes” are off. It just doesn’t work for me in a practical, real-world setting.</p>

<h3 id="the-problem-with-proxies">The Problem with Proxies</h3>

<p>The fundamental issue with AI benchmarks, standardized tests, and even many corporate performance evaluations, is that they rely on proxies. We can’t directly measure intelligence, learning, or the complex value of knowledge work. As in, we truly do not know how to measure these things directly. So we create these stand-ins, these metrics, that we hope will correlate with the real thing.</p>

<p>But these proxies often become distorted. They incentivize narrow optimization, “teaching to the test,” and gaming the system. They create a disconnect between the measured performance and the actual capability. When we forget that these proxies aren’t the real thing we want to measure, we end up with results like we have in education in America. Generations of no meaningful progress.</p>

<h3 id="so-where-does-this-leave-us">So, Where Does This Leave Us?</h3>

<p>The desire for benchmarks in AI feels similar to the education system’s reliance on standardized tests. 
Both are:</p>
<ul>
  <li><strong>Time-Consuming:</strong> They suck up a lot of resources and attention.</li>
  <li><strong>Prone to Perverse Incentives:</strong> They encourage optimizing for the test, not the underlying goal.</li>
  <li><strong>Ultimately, Not That Useful:</strong> They provide a limited, often misleading, picture of actual capability.</li>
</ul>

<p>When generative AI (or whatever comes next) truly reaches a level of general intelligence, we won’t need benchmarks to tell us. We’ll know.</p>

<p>If model providers get sidetracked by teaching to the test and optimizing for benchmarks that would make me bearish for two reasons:</p>
<ol>
  <li>
    <p>They can’t run a tight ship. I’ve never seen a great founder, manager, or teacher focus on teaching to the test (optimizing for the short-term at the expense of the long-term). The model providers who don’t have existing businesses that are cash cows– I’m talking about the Anthropic’s and OpenAI’s, are trying to do something unprecedented. With the goal of AGI/ASI, these companies have to productize the present, while researching and developing the future. That will pull these companies in many directions, which is difficult for even the best run orgs to manage. If one of those directions ends up being benchmark optimization, that would be a waste.</p>
  </li>
  <li>
    <p>If the goal is AGI/ASI, and you’re optimizing for short-term benchmarks, that’s telling me you don’t think you can actually get to the goal.</p>
  </li>
</ol>

<p>In the meantime, as your average AI user, vibe checks are all I need.</p>]]></content><author><name></name></author><category term="jekyll" /><category term="update" /><summary type="html"><![CDATA[]]></summary></entry></feed>